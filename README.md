# ğŸ¤– Asistente Virtual PoC (RAG + LLM Local)

Prueba de Concepto (PoC) de un asistente virtual basado en arquitectura **RAG (Retrieval-Augmented Generation)** utilizando:

- LLM local mediante Ollama
- Sentence Transformers para embeddings
- ChromaDB como base de datos vectorial
- Pipeline de ingestiÃ³n de PDFs y pÃ¡ginas web
- Interfaz conversacional con Streamlit

El sistema permite consultar documentaciÃ³n interna en PDF mediante bÃºsqueda semÃ¡ntica y generaciÃ³n de respuestas contextualizadas.

<img width="789" height="757" alt="image" src="https://github.com/user-attachments/assets/395c38bb-f78e-4ef6-bce1-3f275d1a3634" />

---

## ğŸ“Œ Objetivo del Proyecto

Construir un asistente capaz de:

- Ingerir documentos PDF y web
- Dividirlos en fragmentos semÃ¡nticos (chunks)
- Generar embeddings
- Almacenarlos en una base de datos vectorial
- Recuperar contexto relevante
- Generar respuestas usando un modelo LLM local

---

## ğŸ—ï¸ Arquitectura del Sistema

<img width="5953" height="1719" alt="image" src="https://github.com/user-attachments/assets/934ef1f4-1ede-4087-8636-af417348a314" />

---

## ğŸ“‚ Estructura del Proyecto

```
NTH-POCCHAT/
â”‚
â”œâ”€â”€ app/                          # LÃ³gica principal del asistente
â”‚   â”œâ”€â”€ chat_app.py               
â”‚   â”œâ”€â”€ config_bot.py             
â”‚   â”œâ”€â”€ intent_classifier.py      
â”‚   â””â”€â”€ ollama_client.py         
â”‚
â”œâ”€â”€ ingestion/                    # Pipeline de ingestiÃ³n y procesamiento
â”‚   â”œâ”€â”€ load_pdfs.py              
â”‚   â”œâ”€â”€ clean_pdfs.py             
â”‚   â”œâ”€â”€ chunks_pdfs.py            
â”‚   â”œâ”€â”€ create_embeddings.py      
â”‚   â”‚
â”‚   â””â”€â”€ web/                      
â”‚       â”œâ”€â”€ download_web_pages.py 
â”‚       â””â”€â”€ clean_raw_web.py      
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                      # Datos originales sin procesar
â”‚   â”‚   â”œâ”€â”€ pdfs/                 
â”‚   â”‚   â””â”€â”€ web/                  
â”‚   â”‚
â”‚   â””â”€â”€ processed/                # Datos transformados
â”‚       â”œâ”€â”€ pdfs.json
â”‚       â”œâ”€â”€ pdfs_chunks.json
â”‚       â”œâ”€â”€ pdfs_clean_chunks.json
â”‚       â”œâ”€â”€ web_raw.json
â”‚       â””â”€â”€ web_chunks.json
â”‚
â”œâ”€â”€ vector_store/                 # Base de datos vectorial (Chroma persistente)
â”‚
â”œâ”€â”€ pipeline_pdf.py               # Script principal de pipeline para PDFs
â”œâ”€â”€ pipeline_web.py               # Script principal de pipeline para web
â”œâ”€â”€ config.json                   # ConfiguraciÃ³n general del sistema
â””â”€â”€ .gitignore
```

---

### ğŸ” OrganizaciÃ³n por Capas

El proyecto estÃ¡ estructurado en tres capas principales:

1. **Capa de IngestiÃ³n**
   - ExtracciÃ³n
   - Limpieza
   - Chunking
   - Embeddings

2. **Capa de RecuperaciÃ³n**
   - Vector store (Chroma)
   - BÃºsqueda semÃ¡ntica

3. **Capa de AplicaciÃ³n**
   - Chat
   - ClasificaciÃ³n de intenciÃ³n
   - ConstrucciÃ³n de prompts
   - ComunicaciÃ³n con LLM

---

### ğŸ§  Principio ArquitectÃ³nico

SeparaciÃ³n clara entre:

- Procesamiento offline (ingestiÃ³n)
- RecuperaciÃ³n semÃ¡ntica
- LÃ³gica conversacional
- Infraestructura LLM

Esto permite:

- Reprocesar datos sin afectar la app
- Cambiar modelo LLM sin modificar ingestiÃ³n
- Escalar fuentes (PDF + Web) de forma modular

---

## ğŸ”„ Pipeline de Procesamiento

### 1ï¸âƒ£ IngestiÃ³n de PDFs

- Carga de documentos PDF
- ExtracciÃ³n de texto
- Limpieza bÃ¡sica de formato

---

### 2ï¸âƒ£ Estrategia de Chunking

Los documentos se dividen en fragmentos para:

- Mejorar la precisiÃ³n en recuperaciÃ³n
- Optimizar el uso de ventana de contexto del LLM
- Permitir bÃºsqueda semÃ¡ntica eficiente

ParÃ¡metros configurables:
- TamaÃ±o del chunk
- Solapamiento entre fragmentos

---

### 3ï¸âƒ£ GeneraciÃ³n de Embeddings

Cada fragmento se transforma en un vector numÃ©rico utilizando:

```python
model = SentenceTransformer("all-MiniLM-L6-v2")
```

Estos vectores representan el significado semÃ¡ntico en un espacio de alta dimensiÃ³n.

---

### 4ï¸âƒ£ Filtro de Similitud (EliminaciÃ³n de Duplicados)

Para evitar almacenar contenido redundante, se aplica un filtro basado en similitud coseno:

```python
cosine_similarity(new_embedding, existing_embeddings)
```

Si la similitud â‰¥ 0.90, el fragmento se considera demasiado similar y no se inserta.

Esto evita:

- DuplicaciÃ³n semÃ¡ntica
- Sesgo en recuperaciÃ³n
- Crecimiento innecesario del vector store

---

### 5ï¸âƒ£ Base de Datos Vectorial (ChromaDB)

Se almacenan:

- Embeddings
- Texto original
- Metadatos (PDF origen, pÃ¡gina, etc.)

El almacenamiento es persistente.

---

### 6ï¸âƒ£ Fase de RecuperaciÃ³n

Cuando el usuario formula una pregunta:

1. Se genera su embedding.
2. Se buscan los vectores mÃ¡s similares.
3. Se recuperan los Top-K fragmentos mÃ¡s relevantes.

---

### 7ï¸âƒ£ GeneraciÃ³n de Respuesta

Se construye un prompt que incluye:

- Instrucciones del sistema
- Contexto recuperado
- Pregunta del usuario

---

## ğŸ‘¨â€ğŸ’» Autor

Cristian Marrero  
IngenierÃ­a InformÃ¡tica  

Prueba de concepto de asistente virtual basado en RAG.

---
